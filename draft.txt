import os
import platform
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    AutoConfig,
    BitsAndBytesConfig,
    TrainingArguments,
    default_data_collator,
)
from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer, setup_chat_format
from huggingface_hub import login
from itertools import islice
import logging


### GLOBAL CONFIG ###
model_name = "google/gemma-2-2b-it"
local_file_path = "prepared_dataset.json"
new_model_path = "Gemma-2-9b-it-chat-doctor"
hf_token = "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"

# Cấu hình logging để ghi vào file và console
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("training_log.txt"),  # Ghi log vào file "training_log.txt"
        logging.StreamHandler()  # Ghi log ra console
    ]
)
logger = logging.getLogger()

# Cấu hình môi trường
device = (
    "mps" if torch.backends.mps.is_available() else
    "cuda" if torch.cuda.is_available() else
    "cpu"
)
device = "cpu"


### FUNCTION DEFINITIONS ###

def configure_huggingface():
    """Đăng nhập Hugging Face và thiết lập môi trường."""
    try:
        login(token=hf_token)
        logger.info(f"Using device: {device}")
    except Exception as e:
        logger.error(f"Error during Hugging Face login: {e}")
        raise


def load_and_prepare_dataset(file_path, train_ratio=0.8, shuffle_seed=65, repeat=5):
    """Load và chuẩn bị dataset."""
    try:
        logger.info("Loading dataset...")
        dataset = load_dataset("json", data_files=file_path, split="train", streaming=True)

        # Đếm tổng số dòng trong dataset
        total_size = sum(1 for _ in dataset)  # Đếm số dòng

        # Tính kích thước tập huấn luyện và kiểm tra
        train_size = int(0.8 * total_size)  # 80% cho train
        test_size = total_size - train_size  # 20% còn lại cho test

        # Reset iterator và chia tập dữ liệu
        dataset = load_dataset("json", data_files=file_path, split="train", streaming=True)
        train_dataset = list(islice(dataset, train_size))
        test_dataset = list(islice(dataset, test_size))

        # Tạo DatasetDict
        dataset = DatasetDict({
            "train": Dataset.from_list(train_dataset),
            "test": Dataset.from_list(test_dataset),
        })

        # Shuffle và repeat
        dataset["train"] = dataset["train"].shuffle(seed=shuffle_seed).flatten_indices()
        dataset["train"] = concatenate_datasets([dataset["train"]] * repeat)

        logger.info(f"Training set size: {len(dataset['train'])}")
        logger.info(f"Test set size: {len(dataset['test'])}")
        return dataset
    except Exception as e:
        logger.error(f"Error loading or preparing dataset: {e}")
        raise


def preprocess_data(row):
    row['input'] = str(row['input'])
    row['output'] = str(row['output'])
    row['text'] = f"User: {row['input']}\nAssistant: {row['output']}"
    return row


def clean_dataset(dataset):
    """Loại bỏ các mẫu có giá trị None hoặc không phải chuỗi."""
    def is_valid(row):
        return row["input"] and row["output"]
    
    logger.info(f"Cleaning dataset with {len(dataset)} samples.")
    return dataset.filter(is_valid)


def configure_model(model_name, use_quantization=False, use_remote_code=False):
    """
    Cấu hình mô hình Transformer và chuẩn bị thiết bị.
    """
    try:
        # Phát hiện hệ điều hành
        os_type = platform.system().lower()
        device = "cpu"  # Mặc định là CPU
        torch_dtype = torch.float32  # Mặc định là float32
        attn_implementation = "eager"  # Mặc định cơ chế tính toán attention

        # Cấu hình lượng tử hóa nếu bật
        quantization_config = None
        if use_quantization and torch.cuda.is_available():
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16
            )

        # Kiểm tra khả năng phần cứng và thiết bị
        if os_type == "darwin":  # macOS
            if torch.backends.mps.is_available():
                device = "mps"
                torch_dtype = torch.float16  # MPS hỗ trợ float16
                logger.info("Running on macOS with MPS")
            else:
                logger.warning("macOS detected, but MPS is not available. Using CPU.")
        elif torch.cuda.is_available():  # Linux/Windows với GPU NVIDIA
            device = "cuda"
            if torch.cuda.get_device_capability()[0] >= 8:
                torch_dtype = torch.bfloat16
                attn_implementation = "flash_attention_2"  # Tối ưu với Flash Attention
                logger.info("Using Flash Attention 2 and bfloat16 for optimal performance")
            else:
                torch_dtype = torch.float16
                attn_implementation = "eager"  # Dùng cơ chế mặc định
                logger.info("Using float16 with default attention implementation")
        else:
            logger.warning(f"{os_type.capitalize()} detected. Using CPU.")

        # Quyết định sử dụng device_map hay không
        device_map = "auto" if device in ["cuda", "mps"] else None

        # Tải cấu hình mô hình
        config = AutoConfig.from_pretrained(model_name)
        config.save_pretrained(new_model_path)

        # Tải mô hình
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            trust_remote_code=use_remote_code,
            torch_dtype=torch_dtype,
            device_map=device_map,
            low_cpu_mem_usage=True,
            config=config
        )

        # Tải tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            use_fast=True,
            trust_remote_code=use_remote_code
        )

        # Kiểm tra và thiết lập định dạng hội thoại
        if not hasattr(tokenizer, "chat_template"):
            model, tokenizer = setup_chat_format(model, tokenizer)

        # Không gọi model.to(device) nếu đã sử dụng device_map
        if device_map is None:
            model.to(device)

        logger.info(f"Model loaded on device: {device}")
        logger.info(f"Attention Implementation: {attn_implementation}")
        return model, tokenizer, device, attn_implementation
    except Exception as e:
        logger.error(f"Error during model configuration: {e}")
        raise


def train_model(model, tokenizer, dataset, output_dir, device):
    """Huấn luyện mô hình."""
    try:
        # Cấu hình LoRA (PEFT)
        peft_config = LoraConfig(
            r=16,
            lora_alpha=32,
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM",
        )

        # Gắn PEFT vào mô hình
        model = get_peft_model(model, peft_config)

        # Tắt cache để tương thích với LoRA
        model.config.use_cache = False

        # Cấu hình TrainingArguments
        training_args = TrainingArguments(
            output_dir=output_dir,
            per_device_train_batch_size=1,
            gradient_accumulation_steps=4,
            num_train_epochs=5,
            eval_strategy="steps",
            eval_steps=50,
            logging_steps=10,
            warmup_steps=50,
            learning_rate=2e-5,
            fp16=False,
            bf16=False,
            max_grad_norm=1.0,
            dataloader_num_workers=0,
        )

        # Khởi tạo SFTTrainer
        trainer = SFTTrainer(
            model=model,
            train_dataset=dataset["train"],
            eval_dataset=dataset["test"],
            tokenizer=tokenizer,
            args=training_args,
            max_seq_length=512,
            dataset_text_field="text",
            peft_config=peft_config,
        )

        # Huấn luyện mô hình
        model.train()
        trainer.train()

        logger.info("Training completed.")

        # Lưu mô hình và tokenizer
        model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)

        logger.info(f"Model and tokenizer saved to: {output_dir}")
        return model, trainer
    except Exception as e:
        logger.error(f"Error during training: {e}")
        raise


def test_model_response(prompt, max_length=1500):
    """Kiểm tra phản hồi của mô hình."""
    try:
        input_text = f"User: {prompt}\nAssistant:"
        inputs = tokenizer(input_text, return_tensors="pt").to(device)

        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            do_sample=True,
            temperature=0.7,
            top_p=0.95,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id,
        )
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        assistant_response = generated_text.split("Assistant:")[-1].strip()
        return assistant_response
    except Exception as e:
        logger.error(f"Error during model inference: {e}")
        raise


### MAIN EXECUTION ###

if __name__ == "__main__":
    try:
        # 1. Cấu hình Hugging Face
        configure_huggingface()

        # 2. Cấu hình mô hình và thiết bị (khởi tạo model và tokenizer)
        model, tokenizer, device, attn_implementation = configure_model(model_name)

        # 3. Load và chuẩn bị dataset
        dataset = load_and_prepare_dataset(local_file_path)

        # 4. Làm sạch dữ liệu
        dataset["train"] = clean_dataset(dataset["train"])
        dataset["test"] = clean_dataset(dataset["test"])

        # 5. Tiền xử lý dataset
        dataset = dataset.map(
            preprocess_data,
            num_proc=4
        )

        # 6. Huấn luyện mô hình
        trainer = train_model(model, tokenizer, dataset, new_model_path, device)

        # 7. Kiểm tra mô hình trên dữ liệu huấn luyện
        train_prompt = dataset["train"][0]["input"]
        train_response = test_model_response(train_prompt)
        logger.info(f"Prompt: {train_prompt}")
        logger.info(f"Expected: {dataset['train'][0]['output']}")
        logger.info(f"Model Response: {train_response}")

        # 8. Kiểm tra mô hình trên dữ liệu kiểm tra
        test_prompt = dataset["test"][0]["input"]
        test_response = test_model_response(test_prompt)
        logger.info(f"Prompt: {test_prompt}")
        logger.info(f"Expected: {dataset['test'][0]['output']}")
        logger.info(f"Model Response: {test_response}")

    except Exception as e:
        logger.error(f"Error during execution: {e}")
